{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc75dce8",
   "metadata": {},
   "source": [
    "## Graded Assessment\n",
    "\n",
    "### For questions 1, 2, and 3 \n",
    "\n",
    "- Load the breast cancer dataset from scikit-learn; you can do this by running load_breast_cancer() from sklearn.datasets \n",
    "- The target variable here is whether breast cancer was detected or not (use the .target attribute on your dataset to acquire this) \n",
    "- Attempt logistic regression on this dataset to predict if breast cancer is detected or not using the default solver \n",
    "- Using sklearn.model_selection.train_test_split(), please split the dataset as follows \n",
    "- Use an 80:20 train:test split \n",
    "- Please set random_state = 9001 so that the results you obtain can be compared to ours \n",
    "- Please scale your data using MinMaxScaler() from sklearn.preprocessing \n",
    "- Obtain a classification report containing metrics such as accuracy, precision, and recall on this data; you can use classification_report() from sklearn.metrics to achieve this \n",
    " \n",
    "\n",
    "### For questions 4 and 5 \n",
    "\n",
    "- Load the diabetes dataset from scikit-learn; you can do this by running load_diabetes() from sklearn.datasets \n",
    "- The target variable here is the disease progression score (use the .target attribute on your dataset to acquire this) \n",
    "Attempt linear regression on this dataset to predict the disease progression score; you will perform linear regression with \n",
    "- No regularisation \n",
    "- L1 regularisation (with alpha = 1) \n",
    "- L2 regularisation (with alpha = 1) \n",
    "- Using sklearn.model_selection.train_test_split(), please split the dataset as follows \n",
    "- Use an 80:20 train:test split \n",
    "- Please set random_state = 9001 so that the results you obtain can be compared to ours \n",
    "- Please scale your data using MinMaxScaler() from sklearn.preprocessing \n",
    "- Obtain the mean squared error (MSE) for all three models using the appropriate function from sklearn.metrics \n",
    "- Obtain the coefficients of your fitted models by using their .coef_ attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "259ca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0da2ee",
   "metadata": {},
   "source": [
    "### Breast Cancer Prediction - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f9f12cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the breast cancer dataset from scikit-learn; you can do this by running load_breast_cancer() from sklearn.datasets \n",
    "cancer_data = load_breast_cancer(as_frame=True)\n",
    "df_cancer = cancer_data.frame\n",
    "display(df_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "868576a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the target_names attribute from the dataset to see the names of the classes\n",
    "cancer_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5591ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (569, 30)\n",
      "Class distribution:\n",
      "  malignant: 212 (37.3%)\n",
      "  benign: 357 (62.7%)\n",
      "\n",
      "Class distribution using value_counts():\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "Target = 0 => Malignant\n",
      "Target = 1 => Benign\n"
     ]
    }
   ],
   "source": [
    "# The target variable here is whether breast cancer was detected or not (use the .target attribute on your dataset to acquire this) \n",
    "\n",
    "X = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
    "y = pd.Series(cancer_data.target)\n",
    "\n",
    "print(f'Input shape: {X.shape}')\n",
    "print(f'Class distribution:')\n",
    "for i, class_name in enumerate(cancer_data.target_names):\n",
    "    count = (y == i).sum()\n",
    "    print(f'  {class_name}: {count} ({count/len(y):.1%})')\n",
    "\n",
    "print(\"\\nClass distribution using value_counts():\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"Target = 0 => Malignant\")\n",
    "print(\"Target = 1 => Benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "300b197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt logistic regression on this dataset to predict if breast cancer is detected or not using the default solver \n",
    "logistic_model = LogisticRegression(random_state=9001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b576935c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (455, 30)\n",
      "Test set shape: (114, 30)\n",
      "Training set class distribution: [170 285]\n",
      "Test set class distribution: [42 72]\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn.model_selection.train_test_split(), please split the dataset as follows \n",
    "# Use an 80:20 train:test split \n",
    "# Please set random_state = 9001 so that the results you obtain can be compared to ours \n",
    "\n",
    "# Note: Use stratify=y, to address class imbalance and have same distribution of target \n",
    "# variable in training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=9001)\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}')\n",
    "\n",
    "print(f'Training set class distribution: {np.bincount(y_train)}')\n",
    "print(f'Test set class distribution: {np.bincount(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0980931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please scale your data using MinMaxScaler() from sklearn.preprocessing \n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the logistic regression model on the training data and predict on the test data\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make y predictions on the test data and store them in a variable called y_pred_logistic\n",
    "y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f804ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.93      0.94        42\n",
      "      benign       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "Accuracy: 0.956140350877193\n",
      "Precision: 0.958904109589041\n",
      "Recall: 0.9722222222222222\n",
      "F1 Score: 0.9655172413793104\n",
      "Confusion Matrix:\n",
      " [[39  3]\n",
      " [ 2 70]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain a classification report containing metrics such as accuracy, precision, and recall on this data; \n",
    "# you can use classification_report() from sklearn.metrics to achieve this \n",
    "\n",
    "report = classification_report(y_test, y_pred_logistic, target_names=cancer_data.target_names)\n",
    "print(report)\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred_logistic))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_logistic))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_logistic))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_logistic))\n",
    "cm = confusion_matrix(y_test, y_pred_logistic)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53d20bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 feature  coefficient  abs_coefficient\n",
      "20          worst radius    -2.379557         2.379557\n",
      "27  worst concave points    -2.333994         2.333994\n",
      "21         worst texture    -2.169682         2.169682\n",
      "22       worst perimeter    -2.168315         2.168315\n",
      "23            worst area    -1.940119         1.940119\n",
      "7    mean concave points    -1.783235         1.783235\n",
      "0            mean radius    -1.740449         1.740449\n",
      "2         mean perimeter    -1.712651         1.712651\n",
      "6         mean concavity    -1.514329         1.514329\n",
      "1           mean texture    -1.494990         1.494990\n"
     ]
    }
   ],
   "source": [
    "# Get coefficients\n",
    "coef = logistic_model.coef_[0]\n",
    "\n",
    "# Rank features by absolute coefficient magnitude\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": coef,\n",
    "    \"abs_coefficient\": abs(coef)\n",
    "}).sort_values(by=\"abs_coefficient\", ascending=False)\n",
    "\n",
    "print(coef_df.head(10))  # top 10 strongest features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160cb46",
   "metadata": {},
   "source": [
    "### Dibetis Disease Progression Score Prediction - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33008054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018114</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081413</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019907 -0.017646   151.0  \n",
       "1   -0.039493 -0.068332 -0.092204    75.0  \n",
       "2   -0.002592  0.002861 -0.025930   141.0  \n",
       "3    0.034309  0.022688 -0.009362   206.0  \n",
       "4   -0.002592 -0.031988 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018114  0.044485   104.0  \n",
       "439 -0.011080 -0.046883  0.015491   132.0  \n",
       "440  0.026560  0.044529 -0.025930   220.0  \n",
       "441 -0.039493 -0.004222  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the diabetes dataset from scikit-learn; you can do this by running load_diabetes() from sklearn.datasets \n",
    "\n",
    "diabetes_data = load_diabetes(as_frame=True)\n",
    "df_dibetes  = diabetes_data.frame   \n",
    "display(df_dibetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0343b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (442, 10)\n",
      "Target statistics:\n",
      "  Mean: 152.13\n",
      "  Std: 77.09\n",
      "  Min: 25.00\n",
      "  Max: 346.00\n"
     ]
    }
   ],
   "source": [
    "# The target variable here is the disease progression score (use the .target attribute on your dataset to acquire this) \n",
    "X_diabetes = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y_diabetes = pd.Series(diabetes_data.target)\n",
    "\n",
    "print (f\"Input shape: {X_diabetes.shape}\")\n",
    "print (f\"Target statistics:\")\n",
    "print(f'  Mean: {y_diabetes.mean():.2f}')\n",
    "print(f'  Std: {y_diabetes.std():.2f}')\n",
    "print(f'  Min: {y_diabetes.min():.2f}')\n",
    "print(f'  Max: {y_diabetes.max():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a494faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt linear regression on this dataset to predict the disease progression score; you will perform linear regression with \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "136c970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (353, 10)\n",
      "Test set shape: (89, 10)\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn.model_selection.train_test_split(), please split the dataset as follows \n",
    "# Use an 80:20 train:test split\n",
    "# Please set random_state = 9001 so that the results you obtain can be compared to ours\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(X_diabetes, y_diabetes, test_size=0.2, random_state=9001)\n",
    "print(f'Training set shape: {X_train_diab.shape}')\n",
    "print(f'Test set shape: {X_test_diab.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41b76e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please scale your data using MinMaxScaler() from sklearn.preprocessing \n",
    "scaler_diab = MinMaxScaler()\n",
    "X_train_diab_scaled = scaler_diab.fit_transform(X_train_diab)\n",
    "X_test_diab_scaled = scaler_diab.transform(X_test_diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "370e8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No regularisation \n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_diab_scaled, y_train_diab)\n",
    "y_pred_diab_linear = lr_model.predict(X_test_diab_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "940db60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularisation (with alpha = 1) \n",
    "lasso_model = Lasso(alpha=1, random_state=9001)\n",
    "lasso_model.fit(X_train_diab_scaled, y_train_diab)\n",
    "y_pred_diab_lasso = lasso_model.predict(X_test_diab_scaled) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e779fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularisation (with alpha = 1) \n",
    "ridge_model = Ridge(alpha=1, random_state=9001)\n",
    "ridge_model.fit(X_train_diab_scaled, y_train_diab)\n",
    "y_pred_diab_ridge = ridge_model.predict(X_test_diab_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "caf90f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE):\n",
      "Linear Regression (No regularisation): 2839.66\n",
      "Lasso Regression (L1, alpha=1): 3066.58\n",
      "Ridge Regression (L2, alpha=1): 2914.91\n"
     ]
    }
   ],
   "source": [
    "# Obtain the mean squared error (MSE) for all three models using the appropriate function from sklearn.metrics \n",
    "mse_linear = mean_squared_error(y_test_diab, y_pred_diab_linear)\n",
    "mse_lasso = mean_squared_error(y_test_diab, y_pred_diab_lasso)\n",
    "mse_ridge = mean_squared_error(y_test_diab, y_pred_diab_ridge)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\")\n",
    "print(f\"Linear Regression (No regularisation): {mse_linear:.2f}\")\n",
    "print(f\"Lasso Regression (L1, alpha=1): {mse_lasso:.2f}\")\n",
    "print(f\"Ridge Regression (L2, alpha=1): {mse_ridge:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5164ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Coefficients:\n",
      "\n",
      "Linear Regression Coefficients:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>linear_coef</th>\n",
       "      <th>lasso_coef</th>\n",
       "      <th>ridge_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>-9.017396</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-6.726054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>-19.227353</td>\n",
       "      <td>-7.034736</td>\n",
       "      <td>-18.777937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bmi</td>\n",
       "      <td>124.150965</td>\n",
       "      <td>121.012406</td>\n",
       "      <td>117.778952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bp</td>\n",
       "      <td>81.827676</td>\n",
       "      <td>57.382246</td>\n",
       "      <td>78.640646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s1</td>\n",
       "      <td>-207.799754</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-24.044456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s2</td>\n",
       "      <td>115.732936</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-18.399865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s3</td>\n",
       "      <td>40.323603</td>\n",
       "      <td>-42.473776</td>\n",
       "      <td>-42.662907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s4</td>\n",
       "      <td>60.420372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.573186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s5</td>\n",
       "      <td>178.651061</td>\n",
       "      <td>111.777904</td>\n",
       "      <td>108.938513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s6</td>\n",
       "      <td>25.882092</td>\n",
       "      <td>3.475959</td>\n",
       "      <td>28.734270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  linear_coef  lasso_coef  ridge_coef\n",
       "0     age    -9.017396   -0.000000   -6.726054\n",
       "1     sex   -19.227353   -7.034736  -18.777937\n",
       "2     bmi   124.150965  121.012406  117.778952\n",
       "3      bp    81.827676   57.382246   78.640646\n",
       "4      s1  -207.799754   -0.000000  -24.044456\n",
       "5      s2   115.732936   -0.000000  -18.399865\n",
       "6      s3    40.323603  -42.473776  -42.662907\n",
       "7      s4    60.420372    0.000000   40.573186\n",
       "8      s5   178.651061  111.777904  108.938513\n",
       "9      s6    25.882092    3.475959   28.734270"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the coefficients of your fitted models by using their .coef_ attribute \n",
    "print (f\"Model Coefficients:\")\n",
    "print(f\"\\nLinear Regression Coefficients:\")\n",
    "\n",
    "features_diab = diabetes_data.feature_names\n",
    "coef_linear = lr_model.coef_\n",
    "coef_lasso = lasso_model.coef_\n",
    "coef_ridge = ridge_model.coef_\n",
    "\n",
    "all_coef = pd.DataFrame({\n",
    "            'feature' : features_diab,\n",
    "            'linear_coef': coef_linear,\n",
    "            'lasso_coef' : coef_lasso,\n",
    "            'ridge_coef' : coef_ridge\n",
    "        })\n",
    "\n",
    "display(all_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b8472",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aac94f",
   "metadata": {},
   "source": [
    "**Q1) What is the accuracy of the logistic regression model?**\n",
    "\n",
    "**Answer: 0.96**"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "30474228",
   "metadata": {},
   "source": [
    "**Q2) in breast cancer logistic regression model, assume you have obtained a coefficient of X for the feature symmetry_error. what does this mean?**\n",
    "- a) a one-unit increase in 'symmetry error' decreases the log-odds of the tumour being benign by X \n",
    "- b) a one-unit increase in 'symmetry error' changes the log-odds of the tumour being benign by X \n",
    "- c) a one-unit increase in 'symmetry error' increases the probability of the tunour being benign by X% \n",
    "- d) a one-unit increase in 'symmetry error' increases the tumour's predicted value by X\n",
    "\n",
    "**Answer: b**\n",
    "\n",
    "Logistic Regression Equation\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- Left-hand side = log-odds of the positive class (say, tumour is benign).\n",
    "- Each coefficient 𝛽𝑗 = change in log-odds for a 1-unit increase in feature 𝑥𝑗\n",
    "\n",
    "- a) a one-unit increase in 'symmetry error' decreases the log-odds of the tumour being benign by X\n",
    "- ❌ Not always true. It’s only correct if X is negative. The sign matters.\n",
    "\n",
    "- b) a one-unit increase in 'symmetry error' changes the log-odds of the tumour being benign by X\n",
    "- ✅ Correct. This is the general interpretation of logistic regression coefficients.\n",
    "\n",
    "- c) a one-unit increase in 'symmetry error' increases the probability of the tumour being benign by X%\n",
    "- ❌ Wrong. The effect on probability is nonlinear, depends on the baseline p. You can’t directly say “X%” without context.\n",
    "\n",
    "- d) a one-unit increase in 'symmetry error' increases the tumour's predicted value by X\n",
    "- ❌ Sounds like linear regression, not logistic regression. Logistic regression doesn’t predict directly on the same scale as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd318b99",
   "metadata": {},
   "source": [
    "**Q3) how to find which feature has the highest magnitude coefficient (i.e. strongest influence on the prediction, regardless of sign)**\n",
    "- a) mean fractal dimension \n",
    "- b) area error \n",
    "- c) mean perimeter \n",
    "- d) worst radius\n",
    "\n",
    "**Answer: d**\n",
    "\n",
    "```python\n",
    "# Get coefficients\n",
    "coef = log_reg.coef_[0]\n",
    "\n",
    "# Rank features by absolute coefficient magnitude\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": coef,\n",
    "    \"abs_coefficient\": abs(coef)\n",
    "}).sort_values(by=\"abs_coefficient\", ascending=False)\n",
    "\n",
    "print(coef_df.head(10))  # top 10 strongest features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1f58b",
   "metadata": {},
   "source": [
    "**Q4) which model has the lowest mean squarred error?**\n",
    "- **Linear Regression (No regularisation): 2839.66**\n",
    "- Lasso Regression (L1, alpha=1): 3066.58 \n",
    "- Ridge Regression (L2, alpha=1): 2914.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e6092",
   "metadata": {},
   "source": [
    "**Q5) how many features has lasso eliminated?**\n",
    "\n",
    "**Answer: 4**\n",
    "- Count the features where coefficient is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4219ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a426cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c999460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
